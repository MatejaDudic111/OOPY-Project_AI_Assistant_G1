{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# AI Assistant\n",
    "## Group Members:\n",
    " - Krylova Alena\n",
    " - Dudic Mateja\n",
    " - Saavedra Triana Erwin Omar\n",
    " - Maringer Kelvin\n",
    "\n",
    "## Python Version:\n",
    " - 3.13\n",
    "\n",
    "## Contributions:\n",
    " - Krylova Alena:\n",
    "     - ...\n",
    " - Dudic Mateja:\n",
    "     - ...\n",
    " - Saavedra Triana Erwin Omar:\n",
    "     - ...\n",
    " - Maringer Kelvin:\n",
    "     - ..."
   ],
   "id": "bd15f883796ced19"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# FIRST TIME SETUP\n",
    "# ----------------\n",
    "# MAKE SURE THAT THIS CELL RUNS WITHOUT ERRORS BEFORE PROCEEDING\n",
    "# ----------------"
   ],
   "id": "16272c01b8c7987e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# All the packages that have to be installed should be listed here\n",
    "%pip install numpy pandas matplotlib seaborn kagglehub ipywidgets --quiet\n",
    "# This will filter out the output from Jupyter Notebooks when committing to git, so that diffs are cleaner\n",
    "! git config filter.strip-notebook-output.clean 'jupyter nbconvert --ClearOutputPreprocessor.enabled=True --to=notebook --stdin --stdout --log-level=ERROR'\n",
    "\n",
    "import kagglehub\n",
    "import platform\n",
    "\n",
    "# Download latest version\n",
    "dataset_path = kagglehub.dataset_download(\"prince7489/daily-ai-assistant-usage-behavior-dataset\") + (\"/Daily_AI_Assistant_Usage_Behavior_Dataset.csv\" if platform.system() != \"Windows\" else \"\\\\Daily_AI_Assistant_Usage_Behavior_Dataset.csv\")\n",
    "\n",
    "print(\"Path to dataset files:\", dataset_path)"
   ],
   "id": "e896f8188369ee68"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ----------------------\n",
    "# ----------------------"
   ],
   "id": "b74fab031dec6594"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#All the imports should be listed here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plot\n",
    "import seaborn as sea\n"
   ],
   "id": "9776dc96f35539c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## *Dataset Overview*\n",
    " - Additional Notes etc...\n"
   ],
   "id": "f28858041b25066"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "## Code for the dataset overview Here",
   "id": "202670a5bdcc0674"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## *Data Quality Check*\n",
    " - Additional Notes etc...\n"
   ],
   "id": "b0f27a1210a09e32"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "## Code for the data quality check Here",
   "id": "3c2b86ae29dce109"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## *Data-Preprocessing*\n",
    " - Additional Notes etc..."
   ],
   "id": "41627c5b859e5e66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# From here you can take for the Data Quality Check\n",
    "RAW_data = pd.read_csv(dataset_path)\n",
    "\n",
    "# first we check the number of missing values in each column\n",
    "print(RAW_data.isnull().sum())\n",
    "# after checking we can see that there are no missing values in the dataset\n",
    "\n",
    "# so for the outliers in this dataset , I think the best aproach would be to leave them and just mark them as outliers. In this dataset the outliers might be relevant data from users that have a diferent behavior than the average user, so removing them would mean losing relevant data.\n",
    "\n",
    "# now we will check for outliers using the IQR method\n",
    "\n",
    "y = RAW_data.select_dtypes(include=[np.number])\n",
    "print(y)\n",
    "for column in y:\n",
    "    quartile_min = RAW_data[column].quantile(0.25)\n",
    "    quartile_max = RAW_data[column].quantile(0.75)\n",
    "\n",
    "    IQR = quartile_max - quartile_min\n",
    "\n",
    "    lower_bound = quartile_min - 1.5 * IQR\n",
    "    upper_bound = quartile_max + 1.5 * IQR\n",
    "\n",
    "    outliers_promt_length = RAW_data[(RAW_data[column] < lower_bound) | (RAW_data[column] > upper_bound)].count()\n",
    "    outliers_promt_length = outliers_promt_length.sum()\n",
    "\n",
    "    print(f\"the number of outliers in {column} is the following: \\n{outliers_promt_length}\")\n",
    "\n",
    " # Start of the data preprocessing\n",
    "\n",
    "# after checking we can see that there are no outliers in the dataset, surprinsing but good.\n",
    "# next we will check for duplicates in the dataset\n",
    "x = RAW_data.duplicated().sum()\n",
    "print(f\"the number of duplicates in the dataset is the following: \\n{x}\")\n",
    "# after checking we can see that there are no duplicates in the dataset\n",
    "\n",
    "#\n",
    "\n",
    "# so we continue with creating the required columns\n",
    "\n",
    "\n",
    "# I made an funtion for this part so its easier to read , and taking noticing that the timestamp is an string i decided to slice the string to get the hour part and then convert it to int to compare it\n",
    "def timeOfDay(hour):\n",
    "    if 5 <= hour <= 11:\n",
    "        return \"morning\"\n",
    "    elif 12 <= hour <= 17:\n",
    "        return \"afternoon\"\n",
    "    elif 18 <= hour <= 22:\n",
    "        return \"evening\"\n",
    "    else:\n",
    "        return \"Night\"\n",
    "\n",
    "RAW_data[\"timeOfDay\"] = RAW_data[\"timestamp\"].apply(lambda x:timeOfDay(int(x[11:-6])))\n",
    "RAW_data[\"year\"] = RAW_data[\"timestamp\"].apply(lambda x:int(x[0:4]))\n",
    "\n",
    "# now we are going to convert the columns and timestamp to their proper datatypes\n",
    "\n",
    "RAW_data[\"timestamp\"] = pd.to_datetime(RAW_data[\"timestamp\"])\n",
    "RAW_data[\"device\"] = RAW_data[\"device\"].astype(\"category\")\n",
    "RAW_data[\"assistant_model\"] = RAW_data[\"assistant_model\"].astype(\"category\")\n",
    "RAW_data[\"timeOfDay\"] = RAW_data[\"timeOfDay\"].astype(\"category\")\n",
    "RAW_data[\"usage_category\"] = RAW_data[\"usage_category\"].astype(\"category\")\n",
    "\n",
    "# note that the numericals stay the same, and I decided to leave year as a number\n",
    "\n",
    "RAW_data\n",
    "\n",
    "\n"
   ],
   "id": "a29179a8821b1a45"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## *Data Analysis*\n",
    " - Additional Notes etc..."
   ],
   "id": "40fa4e3e17c9bb6d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(\"helo\")",
   "id": "fe15f1454448cd1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## code for data analysis here\n",
    "print(\"helo\")"
   ],
   "id": "c9288eee8972a0a8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
